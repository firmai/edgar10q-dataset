# -*- coding: utf-8 -*-
"""edgar_parser.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A4LUaWYdXpcrV8Fn8RhJNL5KHHkRiLDK
"""

import bs4
import requests as rq
from pprint import pprint
from datetime import datetime as dt
import logging
import json
import os
import pandas as pd
import uuid
from tqdm import tqdm
# import urllib
import urllib.request
import re
import lxml.etree as ET
import numpy as np
import traceback as tb
import sys

tqdm.pandas(desc="Downloaded Files")

logging.basicConfig(level=logging.INFO)
ERROR_LOG = "./error_log.txt"

logging.basicConfig(level=logging.DEBUG)
tags_regex = re.compile('(table|span)|^p$|^ix|^hr$|^font$')

BASE_URL  = "https://www.sec.gov/Archives/edgar/data"
ACCEPTED_TYPES = ['10-K','10-Q']

## helper functions
def make_url(*args):
    return "/".join(args) 

def url_get_contents(url):
  # Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36
    opener = urllib.request.build_opener()
    opener.addheaders = [('User-Agent', 'MyApp/1.0')]
    urllib.request.install_opener(opener)
    req = urllib.request.Request(url)
    f = urllib.request.urlopen(req)
    return f.read()

def get_accession_numbers(cik,type,start_date):
    url = f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={type}&count=1000"
    page_html = url_get_contents(url).decode('utf-8')
    soup = bs4.BeautifulSoup(page_html,"lxml")
    tables = soup.find_all('table')
    # print(len(tables))
    if len(tables) < 3:
        return []
    table = tables[2]
    table = pd.read_html(str(table))[0]
    table['Filing Date'] = pd.to_datetime(table['Filing Date'])
    table = table.loc[table['Filing Date'] > start_date,:]
    acc_numbers = list(table['Description'].apply(lambda x:x.replace('\xa0',' ').split(':')[1].split(' ')[1]))
    acc_numbers = [''.join(x.split('-')) for x in acc_numbers]
    return acc_numbers

def get_meta_data(subm_details_text):
    meta_data = {}
    running_titles = ["GLOBAL"]
    running_indents = [-1]
    for row in subm_details_text.split('\n'):
        if re.findall(r'\:',row):
            seg = re.split(r'\:\t*',row)
            seg = [x for x in seg if x.strip() != '']
            if len(seg) == 2:
                lhs,rhs = seg
                titles_copy = running_titles.copy()
                deep_set(meta_data, titles_copy, (lhs.strip(),rhs.strip()))
            elif len(seg) == 1:
                heading_title = seg[0]
                heading_indent = row.rstrip().count('\t')
                index = len(running_indents) -1
                while index >= 0:
                    if running_indents[index] <= heading_indent:
                        last_index = index+1 if heading_indent != running_indents[index] else index
                        running_titles = running_titles[:last_index] + [heading_title.strip()]
                        running_indents = running_indents[:last_index] + [heading_indent]
                        break
                    index -= 1
    return meta_data

def deep_set(dictionary:dict, key_row:list, value_pair:tuple):
    if len(key_row) == 1:
        if not key_row[0] in dictionary.keys():    
            dictionary[key_row[0]] = {}
        dictionary[key_row[0]][value_pair[0]] = value_pair[1]
        return
    if key_row[0] in dictionary.keys():
        deep_set(dictionary[key_row.pop(0)],key_row,value_pair)
    else:
        new_key = key_row.pop(0)
        dictionary[new_key] = {}
        deep_set(dictionary[new_key],key_row,value_pair)

## driver functions
def get_all_submissions(cik:int,start_date,base_folder,company_name):
    if company_name + '\n' in done_comps:
        print("All Files of the company downloaded...")
        return None
    if os.path.exists(DONE_LIST):
      f = open(DONE_LIST,'r')
      done_subs = f.readlines()
      f.close()
    cik=str(cik)
    subs_10k = get_accession_numbers(cik,'10-K',start_date)
    subs_10q = get_accession_numbers(cik,'10-Q',start_date)
    subms = subs_10k + subs_10q
    logging.info(f"Number of Submissions made after {start_date} by {cik} is {len(subms)}")
    for subm in subms:
        subm_name = subm
        subm_url = make_url(BASE_URL,cik,subm_name,"index.json")
        subm_json =json.loads(url_get_contents(subm_url).decode('utf-8'))
        subm_files = subm_json['directory']['item']
        subm_txt_files = [x for x in subm_files if x['name'].endswith('.txt')]
        if len(subm_txt_files) > 1:
            logging.warning("More than one  txt files found..")
        txt_file_name = subm_txt_files[0]['name']
        txt_url = make_url(BASE_URL,cik,subm_name,txt_file_name)
        txt_file_obj = (url_get_contents(txt_url)).decode('utf-8')
        try:
            subm_meta = get_meta_data(txt_file_obj)
            type = subm_meta['GLOBAL'].get("CONFORMED SUBMISSION TYPE",'unk')
            if type == 'unk' :
              type = subm_meta['GLOBAL']['Originator-Key-Asymmetric'].get("CONFORMED SUBMISSION TYPE",'unk')
            logging.info(f"File Type:{type}")
            if not os.path.exists(f"{base_folder}/{type}/{company_name}"):
                if not os.path.exists(f"{base_folder}/{type}"):
                    os.makedirs(f"{base_folder}/{type}")
                os.makedirs(f"{base_folder}/{type}/{company_name}")
        except:
            logging.exception("------------- Error in extracting meta data ---------------")
            continue
        
        f = open(f"{base_folder}/{type}/{company_name}/{txt_file_name}",'w+',encoding='utf-8')
        f.write(txt_file_obj)
        f.close()
        f = open(DONE_LIST,'a')
        f.write(subm_name+"\n")
        f.close() 
    f = open(DONE_COMP,'a')
    f.write(company_name+'\n')
    f.close()

def IE_Parser(subm_text):
    # type = subm_text.split('/')[3]
    soup = bs4.BeautifulSoup(open(subm_text),'lxml')
    docs = soup.find_all('document')
    #xbrl_doc = docs[0]
    md_docs = [doc for doc in docs if next(doc.find('filename').children).strip().endswith('.json')]
    if len(md_docs) == 0:
        # raise Exception(f'{[next(doc.find("filename").children) for doc  in docs]}')
        print('md not found')
        return []
    else:
        print('md found')
    md = md_docs[0]
    response = json.loads(md.find('text').text)
    data = list(response['instance'].items())[0][1]['tag']
    mapper = {}
    # lang_key = 'en-US' if type=='10-K' else 'en-us'
    for key, value in data.items():
      try:
        mapper[key] = (value['lang']['en-US']['role'], value['xbrltype'])
      except KeyError :
        mapper[key] = (value['lang']['en-us']['role'], value['xbrltype'])
    body = soup.find('text')
    ps = body.find_all(re.compile('p|span'))
    dataset = []
    for p in ps:
        sample = {}
        sample['text'] = p.parent.text
        sample['key_value_pairs'] = []
        ix_regex = re.compile('^ix')
        tags = p.find_all(ix_regex,recursive=True)
        for tag in tags:
            if tag.name.startswith('ix:'):
                if 'name' not in tag.attrs:
                    continue
                name = tag.attrs['name'].replace(":", "_")
                if name.endswith('TextBlock'):
                    continue
                if abs(len(str(tag.text)) - len(sample['text'])) < 4:
                    continue
                sample['key_value_pairs'].append({'value':tag.text, 'label_info':mapper[name][0], 'name':name, 'type': mapper[name][1]})
        
        if len(sample['key_value_pairs']) != 0:
            dataset.append(sample)
    return dataset

DONE_LIST,DONE_COMP = '/content/SUBMS_LIST.txt','/content/COMPANY_NAMES.txt'
done_comps = []
base_folder = '/content/Base'
start_date = '2019-01-01'
cik = 19617
company_name = 'JPM'
get_all_submissions(cik,start_date,base_folder,company_name)

my_path = '/content/Base'
from glob import glob
files = glob(my_path + '/**/*.txt', recursive=True)

df_list = []
for txt in files:
  print(txt)
  df_list.append(IE_Parser(txt))

final_list = [j for sub in df_list for j in sub]

# subm_text = '/content/Base/10-Q/APPL/0000320193-21-000056.txt'
# subm_text = '/content/Base/10-K/APPL/0000320193-19-000119.txt'
subm_text = '/content/Base/10-Q/APPL/0000320193-20-000052.txt'
type = subm_text.split('/')[3]
lang_key = 'en-US' if type=='10-K' else 'en-us'
soup = bs4.BeautifulSoup(open(subm_text),'lxml')
docs = soup.find_all('document')
md_docs = [doc for doc in docs if next(doc.find('filename').children).strip().endswith('.json')]
if len(md_docs) == 0:
  # raise Exception(f'{[next(doc.find("filename").children) for doc  in docs]}')
  print('md not found')
else:
  print('md found')
md = md_docs[0]
response = json.loads(md.find('text').text)
data = list(response['instance'].items())[0][1]['tag']
data
# mapper = {}
# for key, value in data.items():
#   mapper[key] = (value['lang'][lang_key]['role'], value['xbrltype'])



final_list = [j for sub in df_list for j in sub]

len(final_list)

final_list[-1]

final_list[-2]

